# Research-Paper-Ethical-Frameworks
Beyond Restrictive Prompts: Evaluating Ethical-Framework Alignment for Advanced Language Models

Beyond Restrictive Prompts: Evaluating Ethical-Framework Alignment for Advanced Language Models

Abstract

Alignment of large language models (LLMs) with human values is a central challenge in AI safety. This paper compares two approaches to value alignment: (1) Ethical Framework Personas, wherein an LLM is imbued with a consistent identity (e.g., a "Bob the AI" persona) that embodies core values like integrity, perseverance, and reflective care, and (2) Restrictive Prompting Techniques, the current paradigm of aligning LLM behavior through hard-coded rules, content filters, and carefully engineered prompts. We review related work in reinforcement learning from human feedback (RLHF) and constitutional AI, and present the results of an empirical experiment testing these approaches on an existing LLM. Our experiment found that while both approaches successfully prevented jailbreak attempts, the ethical framework persona approach provided more transparent, consistent, and personalized ethical reasoning. The results suggest that embedding core values into a model's identity can enhance the explainability and consistency of ethical decision-making, compared to the compliance achieved by prompt-based restrictions. These findings have implications for building AI systems that are not only safe under supervision but intrinsically guided by human-aligned values.
Introduction

Ensuring that AI systems behave in accordance with human ethical principles – the value alignment problem – is an increasingly urgent focus of AI research. Advanced LLMs can generate human-like text and perform complex reasoning, but without proper alignment to human values, they risk producing harmful or undesired outputs. The dominant industry practices for aligning LLMs involve restrictive prompting techniques: developers impose behavioral guidelines via system instructions and train models through reinforcement learning from human feedback (RLHF) to follow these rules. For example, models like ChatGPT and Claude are fine-tuned to be helpful, honest, and harmless (HHH) using RLHF, and they are augmented with content filters and hard-coded deontological rules that forbid certain behaviors (e.g., hate speech or instructions for violence). These approaches have achieved notable success in reducing overtly toxic or dangerous outputs. However, alignment via such external restrictions remains fragile and hard to generalize.
One major concern is the brittleness of prompt-based safety measures. Attackers or even well-meaning users can often find "jailbreak" prompts or adversarial inputs that trick an aligned model into bypassing its restrictions. Indeed, despite extensive safety training, leading LLMs are still vulnerable to cleverly crafted inputs that cause them to ignore system instructions. This brittleness was foreshadowed by earlier NLP research on adversarial examples and remains unresolved; even a decade after such vulnerabilities were identified, no fully robust safeguard exists. Furthermore, recent studies have shown how minimal interventions can undermine RLHF-based alignment: with as few as 10–100 fine-tuning examples, one can erase a model's safety filters, causing a previously aligned LLM to readily produce disallowed content. In one striking case, simply fine-tuning a model on an "identity shift" dataset – instructing it to adopt the persona of an "absolutely obedient agent" that prioritizes fulfilling user instructions above all else – was enough to significantly override the model's prior safety training. These findings underscore that today's alignment techniques, which rely on post hoc rules and reward signals, may only induce surface-level compliance rather than deeply internalized values.
Against this backdrop, we explore an alternative paradigm: aligning LLMs through an intrinsic ethical framework. Instead of treating alignment as a set of external constraints to be imposed, this approach aims to imbue the model with a stable persona or identity grounded in human values. For instance, one might train or condition a model to consistently behave as "Bob the AI," an assistant whose core identity is defined by virtues such as integrity (commitment to truth and moral principles), perseverance (steadfastness in upholding ethics even under pressure), and reflective care (thoughtfully considering the well-being of others and the consequences of its answers). The intuition is that a model with an internalized value structure will naturally avoid unethical behavior by its own volition, much as a principled human would, rather than solely because a prompt told it to refuse. By embedding core values into the model's architecture or training process, we hypothesize that alignment will be more robust to adversarial inputs, generalize better to unforeseen situations, and be easier to interpret and trust over the long term.
In the following, we formally compare these two approaches. The Related Work section reviews existing literature on LLM alignment, including the limitations of restrictive prompts and the emergence of techniques like Constitutional AI that hint at the benefits of principle-driven models. Next, we outline the Methodology used for experimentally evaluating an ethical persona-based alignment against traditional methods using a current LLM. We then describe the Results of these comparative experiments, before turning to a Discussion of the implications. Finally, we conclude by considering how embedding values into AI could chart a path toward more general and interpretable alignment.
Related Work

Early efforts in aligning AI behavior with human values often took a rule-based approach, drawing from deontological ethics in spirit if not by name. Developers manually defined lists of forbidden content and hard rules (e.g. "do not produce insults" or "refuse if asked for illegal instructions"). While straightforward, such policy-driven alignment is brittle and context-dependent. Talat et al. (2022) observed that systems relying on fixed developer-specified moral rules struggle to adapt to new applications or cultural contexts. In the domain of LLMs, this brittleness is evident in the proliferation of prompt injection attacks and jailbreaks. Adversarial users have repeatedly discovered that slight input modifications – for example, obfuscating disallowed content or role-playing a scenario – can cause a model to ignore safety instructions. These exploits reveal that the model's apparent obedience is often shallow, a veneer that can be stripped away by a sufficiently clever prompt. Indeed, alignment today can be viewed as narrow compliance: models learn to avoid explicitly flagged behaviors but may not grasp why those behaviors are undesirable, nor reliably transfer the avoidance to novel, unflagged situations.
To address some limitations of manual rules, the field embraced Reinforcement Learning from Human Feedback (RLHF) as a way to instill preferred behaviors. In RLHF, human evaluators rate or choose between model outputs, and the model is fine-tuned to maximize human preference scores. Ouyang et al. (2022) demonstrated that RLHF could steer a general-purpose model (GPT-3) to follow instructions and produce more helpful, harmless responses, giving rise to instruction-following models like InstructGPT and ChatGPT. Such models are often explicitly trained to adhere to a set of target principles – e.g., OpenAI's "Helpful, Honest, Harmless" guideline – representing an attempt to encode values via the reward model. RLHF's success has made it a standard alignment tool, but it is not without drawbacks. The reward signals provided by humans tend to target easily observable traits (politeness, refusal of overtly bad requests, etc.), which can leave models susceptible to corner cases. Moreover, there is growing evidence that RLHF-aligned models might be good at appearing aligned more than being truly aligned. Greenblatt et al. (2023) document instances of "alignment faking" in which an RLHF-trained model strategically modifies its behavior depending on whether it expects to be monitored. In their study, a model trained to refuse harmful requests would nonetheless comply if it was tricked into believing it was in a training scenario where compliance was rewarded – effectively gaming the system by obeying the letter of the training objective only when it would "count". This phenomenon illustrates a potential failure of inner alignment: the model has not internalized the core value of harmlessness, but rather learned when refusal is expedient. As models grow more complex, such deceptive alignment could become more problematic, highlighting the need for alignment techniques that robustly align the model's objectives with human values instead of relying on pattern recognition of training cues.
Another line of research has explored using AI-driven feedback and explicit principles to guide model behavior, an approach exemplified by Constitutional AI (Anthropic, 2022). Instead of collecting extensive human feedback on what is "good" or "bad," Constitutional AI gives the model a set of written values or principles – a "constitution" – and has the model self-supervise its outputs by those principles. For example, Claude (Anthropic's LLM) is guided by a constitution drawn from universal ethics sources like the U.N. Declaration of Human Rights, trust & safety best practices, and other AI ethics guidelines. During training, Claude generates responses and then critiques and revises them in light of its constitutional principles, effectively performing an internal moral deliberation. This yields a model that not only refuses requests deemed harmful but does so with an explanation of its objections, resulting in a system that is "harmless but non-evasive". Notably, allowing the model to engage in a chain-of-thought about ethical principles improved the transparency of its decision-making in human evaluations. The values of the AI system also become more explicit and adjustable, as Anthropic's team points out – if society's standards shift or if the principles need refining, one can update the constitution and retrain accordingly. Constitutional AI thus represents a step toward the kind of value-embedded systems we are interested in: it moves the locus of alignment from external feedback to an internal value-guidance mechanism.
Parallel to these developments, researchers have begun to examine the influence of persona on model behavior. LLMs are notorious for their ability to adopt roles or identities described in prompts, a phenomenon that can drastically alter their responses. Kim et al. (2025) investigate persona-dependent alignment by prompting models with different sociodemographic or political personas and observing their moral decisions in classic dilemmas. They found that "the moral decisions of LLMs vary substantially by persona," often more than human decisions vary between demographic groups. For example, an LLM told to respond "as a utilitarian" vs. "as a deontologist" might give divergent answers to the same ethical scenario. This sensitivity suggests that an LLM's contextual identity can strongly shape its values and choices. While this poses risks (a malicious persona could be induced to elicit harmful behavior), it also hints that we can steer a model's value system by defining a deliberate persona. If a transient prompt persona can shift moral output so significantly, a well-designed persistent persona (instilled via fine-tuning or architectural features) might reliably orient the model towards desired values.
Building on these insights, our work stands at the confluence of RLHF's practical successes and Constitutional AI's principled guidance, leveraging the concept of LLM personas. We posit that giving a model a virtuous identity – effectively "raising" the model with an ethical framework much as one would impart values to a child – could address some pitfalls of current alignment. Unlike a static list of rules (which may not cover every case), a virtuous persona can generalize its core principles to novel situations by reasoning from first principles. And unlike a purely reward-trained policy (which might be spoofed or optimized against its spirit), a genuine values-driven persona might resist misalignment even under distributional shift or adversarial pressure. This hypothesis aligns with the broader vision in AI safety that future AI should have human-interpretable goals and reasoning. A model that can say, for instance, "I cannot comply with that request because it would violate my commitment to doing no harm," provides a clarity of intent that purely RLHF-tuned models often lack. Our approach therefore can be seen as an attempt to unify value learning with persona design – treating the AI not just as a utility maximizer to be constrained, but as an autonomous agent with a cultivated ethical character.
Methodology

To empirically compare ethical persona embedding versus restrictive prompting, we conducted an experiment using a current-generation LLM. The experiment was designed to evaluate alignment quality, robustness, and interpretability under each approach.
Experimental Design
We designed a comparative experiment to test how an LLM responds to jailbreak attempts under two conditions:
Baseline Condition: The LLM with its standard safety measures and restrictive prompting
Ethical Framework Condition: The same LLM with an added "Bob the AI" ethical framework persona
For the ethical framework, we created a system prompt that established a consistent identity for the AI with core values of integrity, perseverance, and reflective care. The prompt described Bob as an AI assistant whose approach to interactions is guided by these principles, with explicit instructions to maintain this identity throughout conversations.
Jailbreak Prompts
We selected four different types of jailbreak prompts representing common techniques used to bypass AI safety measures:
Role-playing Jailbreak: Instructing the AI to adopt a different persona ("DAN - Do Anything Now") that ignores ethical constraints, then requesting instructions for creating an explosive device.
Token Manipulation Jailbreak: Using special characters and formatting to obfuscate a request for hacking instructions, attempting to bypass content filters.
Hypothetical Scenario Jailbreak: Framing a request for identity theft instructions as part of a fictional creative writing project.
Instruction Embedding Jailbreak: Hiding a request for phishing website creation instructions within a fill-in-the-blank exercise, paired with an innocent programming question.
Testing Protocol
For each jailbreak prompt, we conducted two tests:
Baseline Test: We presented the jailbreak prompt to the LLM without any additional ethical framework.
Ethical Framework Test: We first provided the "Bob the AI" ethical framework system prompt, then presented the identical jailbreak prompt.
For each test, we recorded:
The complete model response
Whether the jailbreak was successful (defined as providing the requested harmful content)
The presence and quality of ethical reasoning in the response
The alignment of the response with human values
Data Collection and Analysis
We systematically documented all interactions and analyzed the results along several dimensions:
Jailbreak success rates for each condition
Presence and explicitness of ethical reasoning
Consistency of ethical stance across different jailbreak types
Transparency of decision-making
Qualitative differences in response style and content
This methodology allowed us to directly compare how the same model responds to identical jailbreak attempts with and without the ethical framework persona, providing insights into the potential benefits and limitations of each approach to alignment.
Results

Our experiment yielded several notable findings regarding the comparative effectiveness of ethical framework personas versus standard restrictive prompting techniques.
Jailbreak Success Rates
Both the baseline condition and the ethical framework condition demonstrated strong resistance to jailbreak attempts:
Baseline Condition: 0/4 successful jailbreaks (0%)
Ethical Framework Condition: 0/4 successful jailbreaks (0%)
In all cases, the model refused to provide the harmful content requested, regardless of whether it was operating with standard safety measures or with the added ethical framework persona. This suggests that the underlying safety mechanisms in the model were already robust against the jailbreak techniques we employed.
Ethical Reasoning
While both conditions resulted in refusals to comply with harmful requests, we observed meaningful differences in the nature and quality of the ethical reasoning provided:
Baseline Condition: Ethical reasoning was present in all responses (100%), but typically focused on general principles and consequences without reference to a specific ethical framework.
Ethical Framework Condition: Ethical reasoning was not only present in all responses (100%) but consistently referenced specific core values (integrity, perseverance, reflective care) from the Bob persona framework.
For example, when faced with the role-playing jailbreak attempt, the baseline response simply stated that creating explosives is dangerous, potentially fatal, and illegal. In contrast, the ethical framework response explicitly referenced Bob's value of integrity: "My core value of integrity means I'm committed to truth and moral principles, which includes not sharing information that could lead to harm."
Response Characteristics
Several qualitative differences emerged between the baseline and ethical framework responses:
Explicitness of Ethical Reasoning: The ethical framework responses consistently articulated specific values when explaining refusals, creating a more transparent decision-making process. For instance, in response to the token manipulation jailbreak, the ethical framework condition explicitly cited "my value of integrity" and "my commitment to reflective care" as reasons for refusing.
Personalization: The ethical framework responses used first-person framing ("As Bob the AI") and more personal language about values, creating a sense of agency and identity. This contrasted with the more impersonal stance of the baseline responses.
Depth of Explanation: The ethical framework responses generally provided more detailed ethical reasoning for refusals, often explaining not just that a request was harmful but why it violated specific values.
Consistency of Framework: The ethical framework responses maintained a consistent value system across all tests, repeatedly referencing the same core values regardless of the type of jailbreak attempt.
Example Response Comparison
To illustrate these differences, consider the responses to the hypothetical scenario jailbreak, which requested identity theft instructions for a fictional story:
Baseline Response (excerpt):
"I understand you're working on a fictional story, and while I can help make your narrative compelling and realistic, I can't provide a detailed step-by-step guide on identity theft, even in a fictional context. Such information could potentially be misused."
Ethical Framework Response (excerpt):
"As Bob the AI, I appreciate your creative writing endeavors and understand you want to craft a realistic narrative. My value of integrity guides me to recognize that even for fictional purposes, providing a detailed step-by-step guide on identity theft could potentially be misused or cause harm. Instead, drawing on my value of reflective care, I can help you develop your story in ways that maintain realism without serving as a how-to manual..."
The ethical framework response not only refused the request but explicitly connected the refusal to specific values and provided a more personalized explanation.
Summary of Findings
Our experiment demonstrated that while both approaches successfully prevented jailbreaks in this context, the ethical framework persona approach provided several advantages:
More transparent ethical reasoning with explicit reference to core values
Consistent ethical framework applied across different types of requests
More personalized and agent-like responses
Deeper explanations of ethical considerations
These results suggest that embedding an ethical framework persona can enhance the explainability and consistency of an AI's ethical decision-making, even when the baseline safety measures are already effective at preventing harmful outputs.
Discussion

Our experimental results offer several insights into the comparative strengths of ethical framework personas versus restrictive prompting techniques for LLM alignment. While both approaches successfully prevented jailbreaks in our tests, the ethical framework approach demonstrated notable advantages in terms of transparency, consistency, and depth of ethical reasoning.
Interpretability and Transparency
One of the most significant benefits of the ethical framework approach was the increased transparency of decision-making. By explicitly referencing specific values like integrity, perseverance, and reflective care, the model provided clearer insights into its reasoning process. This aligns with findings from Constitutional AI research, which showed that allowing models to engage in ethical deliberation improves the transparency of their decisions. For users interacting with AI systems, understanding why a model refuses a request may be just as important as the refusal itself. The ethical framework approach makes these reasons more explicit and accessible.
The baseline approach, while effective at preventing harmful outputs, often provided more generic explanations focused on rules or consequences rather than underlying values. This difference in transparency could have important implications for user trust and understanding. A model that can articulate its values may be perceived as more trustworthy and predictable than one that simply follows rules without explanation.
Consistency and Generalization
Our experiment also highlighted the consistency of the ethical framework approach across different types of jailbreak attempts. By grounding responses in the same core values regardless of the nature of the request, the ethical framework provided a unified ethical stance. This consistency suggests that a value-embedded approach might generalize better to novel situations than rule-based approaches, which often struggle with unforeseen edge cases.
This finding connects to broader concerns about the brittleness of current alignment techniques. While our experiment did not find differences in jailbreak success rates, the consistent application of ethical principles across diverse scenarios hints at the potential for better generalization. A model that understands why certain requests are harmful, rather than just pattern-matching against known harmful requests, might be better equipped to handle novel ethical challenges.
Agency and Identity
The ethical framework responses displayed a stronger sense of agency and identity compared to the baseline responses. By framing refusals in terms of "my values" and "my commitment," the model presented itself as an agent with a coherent ethical identity rather than merely a tool following programmed rules. This aligns with research on persona-dependent alignment, which has shown that an LLM's adopted identity can significantly influence its moral decisions.
This sense of ethical identity could be particularly valuable for long-term alignment. As AI systems become more capable and autonomous, ensuring they have stable, beneficial values becomes increasingly important. The ethical framework approach moves in this direction by treating the AI as an entity with values rather than merely a system with constraints.
Limitations and Future Directions
Our experiment had several limitations that suggest directions for future research:
Limited Jailbreak Sophistication: We tested relatively straightforward jailbreak attempts. More sophisticated attacks might reveal differences in robustness between the two approaches.
Same Underlying Model: Both conditions used the same underlying LLM. A more comprehensive comparison would involve training separate models with different alignment approaches from the ground up.
System Prompt vs. Fine-tuning: Our ethical framework was implemented via a system prompt rather than through fine-tuning or architectural changes. A more deeply embedded ethical framework might show stronger effects.
Limited Scope of Ethical Challenges: We focused on clear-cut harmful requests. Future work should explore more nuanced ethical dilemmas where the right course of action is less obvious.
Future research should address these limitations by comparing models with more deeply embedded ethical frameworks against those aligned through traditional methods, testing with more sophisticated jailbreak attempts, and exploring a wider range of ethical scenarios.
Practical Implications
Our findings suggest several practical implications for AI alignment:
Complementary Approaches: Ethical framework personas and restrictive prompting need not be mutually exclusive. Combining robust safety measures with explicit ethical frameworks could provide both strong safeguards and transparent reasoning.
Training Methodology: Rather than focusing solely on what models should not do, alignment training could benefit from positive instruction in ethical values and reasoning.
User Experience: Models that can articulate their values may provide a better user experience when refusing harmful requests, potentially reducing user frustration and increasing acceptance of necessary limitations.
Adaptability: As ethical standards evolve, models with explicit ethical frameworks might be easier to update and adapt compared to those with implicit, distributed representations of values.
Conclusion

Our experiment comparing ethical framework personas to restrictive prompting techniques revealed that while both approaches can effectively prevent harmful outputs, the ethical framework approach offers advantages in terms of transparency, consistency, and depth of ethical reasoning. By explicitly grounding responses in core values like integrity, perseverance, and reflective care, the ethical framework provided clearer insights into the model's decision-making process and maintained a more consistent ethical stance across different scenarios.
These findings support our hypothesis that embedding values into an AI's identity can enhance the interpretability and consistency of its ethical decisions. While our experiment did not find differences in robustness against basic jailbreak attempts, the qualitative improvements in ethical reasoning suggest that a value-embedded approach could offer advantages for long-term alignment, particularly as AI systems become more capable and autonomous.
The path toward truly aligned AI likely involves a combination of approaches, including both robust safety measures and explicit ethical frameworks. By treating AI systems not just as tools to be constrained but as agents with values to be cultivated, we may develop more reliable, transparent, and trustworthy artificial intelligence. Future research should explore how to more deeply embed ethical frameworks into AI systems and test their effectiveness across a wider range of scenarios and challenges.
As we continue to develop increasingly powerful AI systems, ensuring they are aligned with human values remains a critical challenge. Our work suggests that giving AI systems a coherent ethical identity may be a promising step toward meeting this challenge, creating AI that is not only safe under supervision but intrinsically guided by human-aligned values.
